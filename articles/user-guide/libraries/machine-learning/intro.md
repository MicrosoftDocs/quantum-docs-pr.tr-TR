---
title: Kuantum makine öğrenimi kitaplığı
description: Makine öğrenimi 'nin hisse sistemlerinde nasıl kullanıldığını öğrenin
author: alexeib2
ms.author: alexeib
ms.date: 11/22/2019
ms.topic: conceptual
uid: microsoft.quantum.libraries.machine-learning.intro
no-loc:
- Q#
- $$v
ms.openlocfilehash: e2f4a4a63eef40474856426b3b29652b5d3053b2
ms.sourcegitcommit: 71605ea9cc630e84e7ef29027e1f0ea06299747e
ms.translationtype: MT
ms.contentlocale: tr-TR
ms.lasthandoff: 01/26/2021
ms.locfileid: "98854028"
---
# <a name="introduction-to-quantum-machine-learning"></a><span data-ttu-id="8e3fc-103">Hisse Machine Learning giriş</span><span class="sxs-lookup"><span data-stu-id="8e3fc-103">Introduction to Quantum Machine Learning</span></span>

## <a name="framework-and-goals"></a><span data-ttu-id="8e3fc-104">Çerçeve ve hedefler</span><span class="sxs-lookup"><span data-stu-id="8e3fc-104">Framework and goals</span></span>

<span data-ttu-id="8e3fc-105">Hisse ve bilgi işleme, klasik makine öğrenimi sınıflandırıcılarında güçlü bir alternatiftir.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-105">Quantum encoding and processing of information is a powerful alternative to classical machine learning Quantum classifiers.</span></span> <span data-ttu-id="8e3fc-106">Özellikle, işlem kaynağı olarak, ve sınıf çıkarımı için hisse ölçümü kullanarak, özellik sayısına kısa bir değer olan, ortalama kayıt verilerini kodlayabileceğimizi sağlar.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-106">In particular, it allows us to encode data in quantum registers that are concise relative to the number of features, systematically employing quantum entanglement as computational resource and employing quantum measurement for class inference.</span></span>
<span data-ttu-id="8e3fc-107">Devre merkezli hisse ayırıcı, veri kodlamasını hızlı bir şekilde birleştiren ve veri örneklerinin sınıf etiketlerini çıkarması için ölçüm tarafından izlenen görece basit bir hisse atım çözümüdür.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-107">Circuit centric quantum classifier is a relatively simple quantum solution that combines data encoding with a rapidly entangling/disentangling quantum circuit followed by measurement to infer class labels of data samples.</span></span>
<span data-ttu-id="8e3fc-108">Amaç, konu devrelerinin klasik şekilde ve depolama alanının yanı sıra son derece büyük özellik alanları için de devre parametrelerinin karma hisse/klasik eğitimine sahip olmasını sağlamaktır.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-108">The goal is to ensure classical characterization and storage of subject circuits, as well as hybrid quantum/classical training of the circuit parameters even for extremely large feature spaces.</span></span>

## <a name="classifier-architecture"></a><span data-ttu-id="8e3fc-109">Sınıflandırıcı mimarisi</span><span class="sxs-lookup"><span data-stu-id="8e3fc-109">Classifier architecture</span></span>

<span data-ttu-id="8e3fc-110">Sınıflandırma, \{ belirli veri örneklerinin $ y_1, y_2, \lnoktalar, y_d $ sınıf etiketlerini çıkarmaktır \} .</span><span class="sxs-lookup"><span data-stu-id="8e3fc-110">Classification is a supervised machine learning task, where the goal is to infer class labels $\{y_1,y_2,\ldots,y_d\}$ of certain data samples.</span></span> <span data-ttu-id="8e3fc-111">"Eğitim veri kümesi", \{ önceden atanmış bilinen etiketlerle $ \mathcal{D} = (x, y)} $ örnekleri koleksiyonudur.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-111">The "training data set" is a collection of samples $\mathcal{D}=\{(x,y)}$ with known pre-assigned labels.</span></span> <span data-ttu-id="8e3fc-112">Burada $x $ bir veri örneğidir ve $y $ "eğitim etiketi" adı verilen bilinen etikettir.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-112">Here $x$ is a data sample and $y$ is its known label called "training label".</span></span>
<span data-ttu-id="8e3fc-113">Geleneksel yöntemlere benzer şekilde, hisse sınıflandırması üç adımdan oluşur:</span><span class="sxs-lookup"><span data-stu-id="8e3fc-113">Somewhat similar to traditional methods, quantum classification consists of three steps:</span></span>
- <span data-ttu-id="8e3fc-114">veri kodlama</span><span class="sxs-lookup"><span data-stu-id="8e3fc-114">data encoding</span></span>
- <span data-ttu-id="8e3fc-115">sınıflandırıcı durumunun hazırlanması</span><span class="sxs-lookup"><span data-stu-id="8e3fc-115">preparation of a classifier state</span></span>
- <span data-ttu-id="8e3fc-116">ölçüm, ölçümün dayalı doğası nedeniyle, bu üç adım birden çok kez tekrarlanmış olmalıdır.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-116">measurement Due to the probabilistic nature of the measurement, these three steps must be repeated multiple times.</span></span> <span data-ttu-id="8e3fc-117">Hem kodlama hem de sınıflandırıcı durumunun bilgi işlem miktarı *hisse* kullanım yoluyla yapılır.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-117">Both the encoding and the computing of the classifier state are done by means of *quantum circuits*.</span></span> <span data-ttu-id="8e3fc-118">Kodlama devresi genellikle veri odaklı ve parametre-ücretsiz olsa da, sınıflandırıcı devresi uygun bir öğrenme parametreleri kümesi içerir.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-118">While the encoding circuit is usually data-driven and parameter-free, the classifier circuit contains a sufficient set of learnable parameters.</span></span> 

<span data-ttu-id="8e3fc-119">Önerilen çözümde, sınıflandırıcı devresi tek qubit döndürmeler ve iki qubit kontrollü döndürmeler oluşur.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-119">In the proposed solution the classifier circuit is composed of single-qubit rotations and two-qubit controlled rotations.</span></span> <span data-ttu-id="8e3fc-120">Burada öğrenme parametreleri, döndürme açıtlardır.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-120">The learnable parameters here are the rotation angles.</span></span> <span data-ttu-id="8e3fc-121">Döndürme ve denetlenen döndürme kapıları, hisse alma işlemi için *evrensel* olarak bilinir. Bu, tüm Unitary ağırlığı, söz konusu kapıları oluşan uzun bir devreye göre parçalanabileceği anlamına gelir.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-121">The rotation and controlled rotation gates are known to be *universal* for quantum computation, which means that any unitary weight matrix can be decomposed into a long enough circuit consisting of such gates.</span></span>

<span data-ttu-id="8e3fc-122">Önerilen sürümde, yalnızca tek bir sıklık tahmini tarafından izlenen yalnızca bir devre desteklenir.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-122">In the proposed version, only one circuit followed by a single frequency estimation is supported.</span></span>
<span data-ttu-id="8e3fc-123">Bu nedenle çözüm, düşük dereceli bir polinom çekirdeği olan destek vektör makinesi 'nin bir hisse andır.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-123">Thus, the solution is a quantum analog of a support vector machine with a low-degree polynomial kernel.</span></span>

![Multilayer Perceptron vs. devre merkezli sınıflandırıcı](~/media/DLvsQCC.png)

<span data-ttu-id="8e3fc-125">Basit bir hisse sınıflandırıcı tasarımı, geleneksel destek vektör makinesi (SVM) çözümüyle karşılaştırılabilir.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-125">A simple quantum classifier design can be compared to a traditional support vector machine (SVM) solution.</span></span> <span data-ttu-id="8e3fc-126">SVM 'nin büyük/küçük bir bir çekirdek biçimi olan $ \sum \ alpha_j k (x_j, x) $k $ gibi bir veri $x örneği için çıkarım, belirli bir çekirdek işlevidir.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-126">The inference for a data sample $x$ in case of SVM is done using an optimal kernel form $\sum \alpha_j  k(x_j,x)$ where $k$ is a certain kernel function.</span></span>

<span data-ttu-id="8e3fc-127">Buna karşılık, bir hisse bir sınıflandırıcı $p (y │ x, U (\teta)) = 〈 U (\teta) x | M | U (\teta) x 〉 $ kullanır ve bu da daha da benzer ancak teknik açıdan oldukça farklıdır.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-127">By contrast, a quantum classifier uses the predictor $p(y│x,U(\theta))=〈U(\theta)x|M|U(\theta)x〉$, which is similar in spirit but technically quite different.</span></span> <span data-ttu-id="8e3fc-128">Bu nedenle, basit bir genlik kodlaması kullanıldığında, $p (y │ x, U (\teta)) $, $x $ ' ın yükseltilmiş tudes biçiminde bir ikinci dereceden formdur, ancak bu formun katmaları artık bağımsız olarak öğrenilmemelidir; Bunlar, genellikle, "$" vekt$x örünün boyutundan büyük ölçüde daha az öğrendiği $ \teta $ parametrelerine sahip olan devre $U (\teta) $ öğesinin matris öğelerinden toplanır.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-128">Thus, when a straightforward amplitude encoding is used,  $p(y│x,U(\theta))$ is a quadratic form in the amplitudes of $x$, but the coefficients of this form are no longer learned independently; they are instead aggregated from the matrix elements of the circuit $U(\theta)$, which typically has significantly fewer learnable parameters $\theta$ than the dimension of the vector $x$.</span></span> <span data-ttu-id="8e3fc-129">Özgün özelliklerde $p polinom derecesi (y │ x, U (\teta)) $, $x $ $l $ kopyaları üzerinde hisse bir ürün kodlaması kullanılarak $2 ^ l $ değerine artırılabilir.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-129">The polynomial degree of $p(y│x,U(\theta))$ in the original features can be increased to $2^l$ by using a quantum product encoding on $l$ copies of $x$.</span></span>

<span data-ttu-id="8e3fc-130">Mimarimiz görece basit devreleri araştırır. bu nedenle, tüm aralıklarda veri özellikleri arasındaki tüm bağıntıları yakalamak için *hızlı* bir şekilde olmalıdır.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-130">Our architecture explores relatively shallow circuits, which therefore must be *rapidly entangling* in order to capture all the correlations between the data features at all ranges.</span></span> <span data-ttu-id="8e3fc-131">Aşağıdaki şekilde, en faydalı hızlı bir şekilde, devtoze devre bileşeni örneği gösterilmektedir.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-131">An example of the most useful rapidly entangling circuit component is shown on figure below.</span></span> <span data-ttu-id="8e3fc-132">Bu geometriye sahip bir devre yalnızca $3 n + 1 $ kapısından oluşuyor olsa da, hesapladığı Unitary ağırlığı, $2 ^ n $ özellikleri arasında önemli bir çapraz konuşmayı sağlar.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-132">Even though a circuit with this geometry consists of only $3 n+1$ gates, the unitary weight matrix that it computes ensures significant cross-talk between $2^n$ features.</span></span>

![5 qubit üzerinde hızlı bir şekilde hisse atıştırın (iki döngüsel katman ile).](~/media/5-qubit-qccc.png)

<span data-ttu-id="8e3fc-134">Yukarıdaki örnekteki devre 6 tek qubit kapıları $ (G_1, \lnoktalar, G_5; G_ {16} ) $ ve 10 2-qubits kapıları $ (G_6, \lnoktalar, g_ {15} ) $.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-134">The circuit in the above example consists of 6 single-qubit gates $(G_1,\ldots,G_5; G_{16})$ and 10 two-qubits gates $(G_6,\ldots,G_{15})$.</span></span> <span data-ttu-id="8e3fc-135">Her bir geçit öğrendiğimiz bir parametreyle tanımlanmış olduğu varsayıldığında, 16 öğrendiğimiz parametrelere sahip olduğumuz ve 5-qubit tepk alanının boyutu 32 olur.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-135">Assuming that each of the gates is defined with one learnable parameter we have 16 learnable parameters, while the dimension of the 5-qubit Hilbert space is 32.</span></span> <span data-ttu-id="8e3fc-136">Bu tür devre geometrisi, her türlü $-qubit kaydına kolayca genelleştirilerek, $n $ tek olduğunda, $2 ^ n $ boyutlu özellik alanı için $3 n + 1 $ parametreli bir $n.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-136">Such circuit geometry can be easily generalized to any $n$-qubit register, when $n$ is odd, yielding circuits with $3 n+1$ parameters for $2^n$-dimensional feature space.</span></span>

## <a name="classifier-training-as-a-supervised-learning-task"></a><span data-ttu-id="8e3fc-137">Denetimli bir öğrenme görevi olarak sınıflandırıcı eğitimi</span><span class="sxs-lookup"><span data-stu-id="8e3fc-137">Classifier training as a supervised learning task</span></span>

<span data-ttu-id="8e3fc-138">Bir sınıflandırıcı modelinin eğitimi, işlem parametrelerinin en iyi değerlerini bulmayı içerir, bu nedenle eğitim örnekleri genelinde doğru eğitim etiketlerini erteleme olasılığını en üst düzeye çıkarır.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-138">Training of a classifier model involves finding optimal values of its operational parameters, such that they maximize the average likelihood of inferring the correct training labels across the training samples.</span></span>
<span data-ttu-id="8e3fc-139">Burada, yalnızca iki düzey sınıflandırmayla kendimize sorun yaptık, yani $d = $2 ve yalnızca $y _1, y_2 $ gibi etiketlere sahip iki sınıf vardır.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-139">Here, we concern ourselves with two level classification only, i.e. the case of $d=2$ and only two classes with the labels $y_1,y_2$.</span></span>

> [!NOTE]
> <span data-ttu-id="8e3fc-140">Yöntemlerimizi rastgele sayıda sınıfa Genelleştirmenin bir yolu, qudits ile qubit, yani $d $ tabanlı durumlarıyla ve iki yönlü ölçümü ile $d $ yönlü ölçümle aynı şekilde.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-140">A principled way of generalizing our methods to arbitrary number of classes is to replace qubits with qudits, i.e. quantum units with $d$ basis states, and the two-way measurement with $d$-way measurement.</span></span>

### <a name="likelihood-as-the-training-goal"></a><span data-ttu-id="8e3fc-141">Eğitim hedefi olma olasılığı</span><span class="sxs-lookup"><span data-stu-id="8e3fc-141">Likelihood as the training goal</span></span>

<span data-ttu-id="8e3fc-142">Öğrenime hisse için $U (\teta) $, $ \teta $ parametrelerinin bir vektörü olduğu ve $M $ ile son ölçümü belirten, doğru etiket çıkarımını ortalama olasılığı olan $ $ \begin{hizalaması} \mathcal{L} (\teta) = \frac {1} {| \mathcal{D} |} \left (\ sum_ {(x, y_1) \In\mathcal{D}} P (M = y_1 | U (\teta) x) + \ sum_ {(x, y_2) \in\mathcal{D}} P (M = y_2 | U (\teta) x) \right) \end{hizalaması} $ $ burada $P (M = y | z) $, $y $ ' in hisse durum $z $ ' i ölçmesi olasılığının bir olasılığıdır.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-142">Given a learnable quantum circuit $U(\theta)$, where $\theta$ is a vector of parameters, and denoting the final measurement by $M$, the average likelihood of the correct label inference is $$ \begin{align} \mathcal{L}(\theta)=\frac{1}{|\mathcal{D}|} \left( \sum_{(x,y_1)\in\mathcal{D}} P(M=y_1|U(\theta) x) + \sum_{(x,y_2)\in\mathcal{D}} P(M=y_2|U(\theta) x)\right) \end{align} $$ where $P(M=y|z)$ is the probability of measuring $y$ in quantum state $z$.</span></span>
<span data-ttu-id="8e3fc-143">Burada, $ \mathcal{L} (\teta) $ olasılığının $ \teta $ ' da düzgün olduğunu ve herhangi bir $ \ theta_j $ ' deki türevi, olasılık işlevinin kendisi için kullanılan aynı hisse protokolde olan temelde hesaplanarak hesaplanır.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-143">Here, it suffices to understand that the likelihood function $\mathcal{L}(\theta)$ is smooth in $\theta$ and its derivative in any $\theta_j$ can be computed by essentially the same quantum protocol as used for computing the likelihood function itself.</span></span> <span data-ttu-id="8e3fc-144">Bu, gradyan tarafından $ \mathcal{L} (\teta) $ için optimize etmenizi sağlar.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-144">This allows for optimizing the $\mathcal{L}(\theta)$ by gradient descent.</span></span>

### <a name="classifier-bias-and-training-score"></a><span data-ttu-id="8e3fc-145">Sınıflandırıcı sapması ve eğitim puanı</span><span class="sxs-lookup"><span data-stu-id="8e3fc-145">Classifier bias and training score</span></span>

<span data-ttu-id="8e3fc-146">$ \Teta $ içindeki parametrelerin bazı ara (veya son) değerleri verildiğinde, çıkarımı *yapmak için tek* bir gerçek değer saptamız gerekir $b $.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-146">Given some intermediate (or final) values of the parameters in $\theta$, we need to identify a single real value $b$ know as *classifier bias* to do the inference.</span></span> <span data-ttu-id="8e3fc-147">Etiket çıkarımı kuralı aşağıdaki gibi çalışmaktadır:</span><span class="sxs-lookup"><span data-stu-id="8e3fc-147">The label inference rule works as follows:</span></span> 
- <span data-ttu-id="8e3fc-148">Bir örnek $x $ etiketi, $y $ IF ve yalnızca $P (d = y_2) olarak atanır. U (\teta) x) + b > $0,5 (RULE1) (Aksi takdirde, $y _1 $) etiketi atanır</span><span class="sxs-lookup"><span data-stu-id="8e3fc-148">A sample $x$ is assigned label $y_2$ if and only if $P(M=y_2|U(\theta) x) + b > 0.5$  (RULE1) (otherwise it is assigned label $y_1$)</span></span>

<span data-ttu-id="8e3fc-149">Açıkça $b $, anlamlı olması için $ (-0,5, + 0,5) $ aralığında olmalıdır.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-149">Clearly $b$ must be in the interval $(-0.5,+0.5)$ to be meaningful.</span></span>

<span data-ttu-id="8e3fc-150">RULE1 başına $x $ için gösterilen etiket aslında $y $ öğesinden farklıysa, \mathcal{D} $ içindeki *bir eğitim* durumu $ (x, y), sapma $b $ olarak kabul edilir.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-150">A training case $(x,y) \in \mathcal{D}$ is considered a *misclassification* given the bias $b$ if the label inferred for $x$ as per RULE1 is actually different from $y$.</span></span> <span data-ttu-id="8e3fc-151">Hatalı sınıflandırmaların genel sayısı, $b $ sapması verilen sınıflandırıcının *eğitim puanı* olur.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-151">The overall number of misclassifications is the *training score* of the classifier given the bias $b$.</span></span> <span data-ttu-id="8e3fc-152">*En iyi* sınıflandırıcı sapması $b $ eğitim Puanını en aza indirir.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-152">The *optimal* classifier bias $b$ minimizes the training score.</span></span> <span data-ttu-id="8e3fc-153">Önceden hesaplanan olasılık tahminlerinin $ \{ P (M = y_2 | olduğunu görmek kolaydır | U (\teta) x) | (x, \*) \in\mathcal{D} \} $, en iyi sınıflandırıcı farkı, en fazla $ \ Log_2 (| \mathcal{D} |) yaparak $ (-0,5, + 0,5) $ aralığında ikili arama tarafından bulunabilir. $ adımları.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-153">It is easy to see that, given the precomputed probability estimates $\{ P(M=y_2|U(\theta) x) | (x,\*)\in\mathcal{D} \}$, the optimal classifier bias can be found by binary search in interval $(-0.5,+0.5)$ by making at most $\log_2(|\mathcal{D}|)$ steps.</span></span>

### <a name="reference"></a><span data-ttu-id="8e3fc-154">Başvuru</span><span class="sxs-lookup"><span data-stu-id="8e3fc-154">Reference</span></span>

<span data-ttu-id="8e3fc-155">Bu bilgiler, kodla yürütmeye başlamak için yeterli olmalıdır.</span><span class="sxs-lookup"><span data-stu-id="8e3fc-155">This information should be enough to start playing with the code.</span></span> <span data-ttu-id="8e3fc-156">Ancak, bu model hakkında daha fazla bilgi edinmek istiyorsanız lütfen orijinal teklifi okuyun: [ *' devre merkezli hisse ıcılar ', Maria Schuld, Alex Bocharov, Kronysta svore ve Nathan Wiebe*](https://arxiv.org/abs/1804.00633)</span><span class="sxs-lookup"><span data-stu-id="8e3fc-156">However, if you want to learn more about this model, please read the original proposal: [*'Circuit-centric quantum classifiers', Maria Schuld, Alex Bocharov, Krysta Svore and Nathan Wiebe*](https://arxiv.org/abs/1804.00633)</span></span>

<span data-ttu-id="8e3fc-157">Sonraki adımlarda göreceğiniz kod örneğine ek olarak, [Bu öğreticide](https://github.com/microsoft/QuantumKatas/tree/main/tutorials/QuantumClassification) hisse sınıflandırması 'nı keşfetmeye de başlayabilirsiniz</span><span class="sxs-lookup"><span data-stu-id="8e3fc-157">In addition to the code sample you will see in the next steps, you can also start exploring quantum classification in [this tutorial](https://github.com/microsoft/QuantumKatas/tree/main/tutorials/QuantumClassification)</span></span> 

---
title: Hisse makine öğrenimi kitaplığı sözlüğü
author: alexeib2
ms.author: alexei.bocharov@microsoft.com
ms.date: 2/27/2020
ms.topic: article
uid: microsoft.quantum.libraries.machine-learning.training
no-loc:
- Q#
- $$v
ms.openlocfilehash: 068fc61d0d7c066df1270384679e13a3b3a8c878
ms.sourcegitcommit: 75c4edc7c410cc63dc8352e2a5bef44b433ed188
ms.translationtype: MT
ms.contentlocale: tr-TR
ms.lasthandoff: 08/25/2020
ms.locfileid: "88863038"
---
# <a name="quantum-machine-learning-glossary"></a><span data-ttu-id="8257a-102">Hisse Machine Learning sözlüğü</span><span class="sxs-lookup"><span data-stu-id="8257a-102">Quantum Machine Learning glossary</span></span>

<span data-ttu-id="8257a-103">Devre merkezli bir hisse sınıflandırıcının eğitimi, çok sayıda hareketli parçaya sahip olan ve geleneksel sınıflandırıcılara yönelik eğitim olarak denemeye ve hataya göre aynı (veya biraz daha büyük) bir işlem yapılmasını gerektiren bir işlemdir.</span><span class="sxs-lookup"><span data-stu-id="8257a-103">Training of a circuit-centric quantum classifier is a process with many moving parts that require the same (or slightly larger) amount of calibration by trial and error as training of traditional classifiers.</span></span> <span data-ttu-id="8257a-104">Burada, bu eğitim sürecinin ana kavramlarını ve bu malzemeleri tanımlayacağız.</span><span class="sxs-lookup"><span data-stu-id="8257a-104">Here we define the main concepts and ingredients of this training process.</span></span>

## <a name="trainingtesting-schedules"></a><span data-ttu-id="8257a-105">Eğitim/test zamanlamaları</span><span class="sxs-lookup"><span data-stu-id="8257a-105">Training/testing schedules</span></span>

<span data-ttu-id="8257a-106">Sınıflandırıcı eğitiminde bir *zamanlama* , bir genel eğitim veya test kümesindeki veri örneklerinin bir alt kümesini açıklar.</span><span class="sxs-lookup"><span data-stu-id="8257a-106">In the context of classifier training a *schedule* describes a subset of data samples in an overall training or testing set.</span></span> <span data-ttu-id="8257a-107">Bir zamanlama genellikle örnek dizinlerin bir koleksiyonu olarak tanımlanır.</span><span class="sxs-lookup"><span data-stu-id="8257a-107">A schedule is usually defined as a collection of sample indices.</span></span>

## <a name="parameterbias-scores"></a><span data-ttu-id="8257a-108">Parametre/sapma puanları</span><span class="sxs-lookup"><span data-stu-id="8257a-108">Parameter/bias scores</span></span>

<span data-ttu-id="8257a-109">Bir aday parametresi vektörü ve bir sınıflandırıcı sapması verildiğinde, *doğrulama puanı* seçilen doğrulama zamanlamalarına göre ölçülür ve zamanlamaya göre tüm örneklerde sayılan bir dizi hatalı sınıflandırmalarla ifade edilir.</span><span class="sxs-lookup"><span data-stu-id="8257a-109">Given a candidate parameter vector and a classifier bias, their *validation score* is measured relative to a chosen validation schedule S and is expressed by a number of misclassifications counted over all the samples in the schedule S.</span></span>

## <a name="hyperparameters"></a><span data-ttu-id="8257a-110">Ayarlama hiperparametreleri</span><span class="sxs-lookup"><span data-stu-id="8257a-110">Hyperparameters</span></span>

<span data-ttu-id="8257a-111">Model eğitimi işlemi, *hiper parametreler*adlı bazı önceden ayarlanmış değerlere tabidir:</span><span class="sxs-lookup"><span data-stu-id="8257a-111">The model training process is governed by certain pre-set values called *hyperparameters*:</span></span>

### <a name="learning-rate"></a><span data-ttu-id="8257a-112">Öğrenme oranı</span><span class="sxs-lookup"><span data-stu-id="8257a-112">Learning rate</span></span>

<span data-ttu-id="8257a-113">Bu, anahtar hiper parametrelerinden biridir.</span><span class="sxs-lookup"><span data-stu-id="8257a-113">It is one of the key hyperparameters.</span></span> <span data-ttu-id="8257a-114">Geçerli stochastik gradyan tahmini 'nin parametre güncelleştirmesini ne kadar etkiler olduğunu tanımlar.</span><span class="sxs-lookup"><span data-stu-id="8257a-114">It defines how much current stochastic gradient estimate impacts the parameter update.</span></span> <span data-ttu-id="8257a-115">Parametre güncelleştirme Delta boyutu öğrenme oranı ile orantılıdır.</span><span class="sxs-lookup"><span data-stu-id="8257a-115">The size of parameter update delta is proportional to the learning rate.</span></span> <span data-ttu-id="8257a-116">Daha az öğrenme oranı değerleri, daha yavaş parametre evrimine ve daha yavaş yakınsama olmasına neden olur, ancak büyük oranda büyük bir değer, Gradyan ' ın belirli bir yerel en düşük düzeyde hiç tamamlanmadığından yakınsama işlemi tamamen bozulabilir.</span><span class="sxs-lookup"><span data-stu-id="8257a-116">Smaller learning rate values lead to slower parameter evolution and slower convergence, but excessively large values of LR may break the convergence altogether as the gradient descent never commits to a particular local minimum.</span></span> <span data-ttu-id="8257a-117">Öğrenme oranı eğitim algoritması tarafından bir ölçüde uyarlanırken, için iyi bir başlangıç değeri seçilmesi önemlidir.</span><span class="sxs-lookup"><span data-stu-id="8257a-117">While learning rate is adaptively adjusted by the training algorithm to some extent, selecting a good initial value for it is important.</span></span> <span data-ttu-id="8257a-118">Öğrenme oranı için olağan varsayılan başlangıç değeri 0,1 ' dir.</span><span class="sxs-lookup"><span data-stu-id="8257a-118">A usual default initial value for learning rate is 0.1.</span></span> <span data-ttu-id="8257a-119">Öğrenme oranının en iyi değerini seçmek, ince bir örnektir (örneğin, Goodfellow et bölümünün 4,3 bölümüne bakın., "derin öğrenme", MıT Press, 2017).</span><span class="sxs-lookup"><span data-stu-id="8257a-119">Selecting the best value of learning rate is a fine art (see, for example, section 4.3 of Goodfellow et al.,"Deep learning", MIT Press, 2017).</span></span>

### <a name="minibatch-size"></a><span data-ttu-id="8257a-120">Mini yığın boyutu</span><span class="sxs-lookup"><span data-stu-id="8257a-120">Minibatch size</span></span>

<span data-ttu-id="8257a-121">Tek bir Stokastik gradyanının tahmini için kaç veri örneği kullanıldığını tanımlar.</span><span class="sxs-lookup"><span data-stu-id="8257a-121">Defines how many data samples is used for a single estimation of stochastic gradient.</span></span> <span data-ttu-id="8257a-122">Mini yığın boyutunun daha büyük değerleri genellikle daha sağlam ve daha fazla monoton 'e yol açabilir, ancak herhangi bir gradyan tahmininin maliyeti minimatch boyutuyla orantılıdır.</span><span class="sxs-lookup"><span data-stu-id="8257a-122">Larger values of minibatch size generally lead to more robust and more monotonic convergence but can potentially slow down the training process, as the cost of any one gradient estimation is proportional to the minimatch size.</span></span> <span data-ttu-id="8257a-123">Mini toplu iş boyutu için olağan bir varsayılan değer 10 ' dur.</span><span class="sxs-lookup"><span data-stu-id="8257a-123">A usual default value for the minibatch size is 10.</span></span>

### <a name="training-epochs-tolerance-gridlocks"></a><span data-ttu-id="8257a-124">Eğitim dönemleri, tolerans, gridkilitleri</span><span class="sxs-lookup"><span data-stu-id="8257a-124">Training epochs, tolerance, gridlocks</span></span>

<span data-ttu-id="8257a-125">"Dönem", zamanlanan eğitim verilerinden bir bütün geçiş anlamına gelir.</span><span class="sxs-lookup"><span data-stu-id="8257a-125">"Epoch" means one complete pass through the scheduled training data.</span></span>
<span data-ttu-id="8257a-126">Eğitim iş parçacığı başına maksimum dönemler sayısı (aşağıya bakın)</span><span class="sxs-lookup"><span data-stu-id="8257a-126">The maximum number of epochs per a training thread (see below) should be capped.</span></span> <span data-ttu-id="8257a-127">Eğitim iş parçacığı, en fazla dönemler yürütüldüğünde, sonlanacak (en iyi bilinen aday parametreleriyle birlikte) için tanımlanır.</span><span class="sxs-lookup"><span data-stu-id="8257a-127">The training thread is defined to terminate (with the best known candidate parameters) when the maximum number of epochs has been executed.</span></span> <span data-ttu-id="8257a-128">Ancak, bu tür bir eğitim daha önce doğrulama zamanlamasında yanlış sınıflandırma oranı seçilen bir toleransın altına düştüğünde daha önce sonlandırılır.</span><span class="sxs-lookup"><span data-stu-id="8257a-128">However such training would terminate earlier when misclassification rate on validation schedule falls below a chosen tolerance.</span></span> <span data-ttu-id="8257a-129">Örneğin, yanlış sınıflandırma toleransı 0,01 (%1%) olduğunu varsayalım; 2000 örnek doğrulama kümesinde 20 ' den az sınıflandırma gördüğünüzü kabul ediyorsanız, tolerans düzeyi elde edilir.</span><span class="sxs-lookup"><span data-stu-id="8257a-129">Suppose, for example, that misclassification tolerance is 0.01 (1%); if on validation set of 2000 samples we are seeing fewer than 20 misclassifications, then the tolerance level has been achieved.</span></span> <span data-ttu-id="8257a-130">Aday modelin doğrulama puanı birkaç ardışık dönemler (Gridlock) üzerinde herhangi bir geliştirme göstermezse, eğitim iş parçacığı de zamanından önce sonlanır.</span><span class="sxs-lookup"><span data-stu-id="8257a-130">A training thread also terminates prematurely if the validation score of the candidate model has not shown any improvement over several consecutive epochs (a gridlock).</span></span> <span data-ttu-id="8257a-131">Sapmanızı sonlandırmasının mantığı şu anda sabit olarak kodlanmıştır.</span><span class="sxs-lookup"><span data-stu-id="8257a-131">The logic for the gridlock termination is currently hardcoded.</span></span>

### <a name="measurements-count"></a><span data-ttu-id="8257a-132">Ölçüm sayısı</span><span class="sxs-lookup"><span data-stu-id="8257a-132">Measurements count</span></span>

<span data-ttu-id="8257a-133">Eğitim/doğrulama puanlarını ve bir hisse uygun gözlemlenenler birden fazla ölçüm gerektiren hisse miktarı ile ilgili olarak bir hisse cihazı miktarındaki Stokastik gradyanının bileşenlerini tahmin etme.</span><span class="sxs-lookup"><span data-stu-id="8257a-133">Estimating the training/validation scores and the components of the stochastic gradient on a quantum device amounts to estimating quantum state overlaps that requires multiple measurements of the appropriate observables.</span></span> <span data-ttu-id="8257a-134">Ölçüm sayısı $O (1/\ Epsilon ^ 2) $ $ \epsilon $, istenen tahmin hatası olarak ölçeklendirmelidir.</span><span class="sxs-lookup"><span data-stu-id="8257a-134">The number of measurements should scale as $O(1/\epsilon^2)$ where $\epsilon$ is the desired estimation error.</span></span>
<span data-ttu-id="8257a-135">Thumb kuralı olarak, ilk ölçüm sayısı yaklaşık $1/\ mbox {toleransı} ^ 2 $ (önceki paragrafta tolerans tanımına bakın) olabilir.</span><span class="sxs-lookup"><span data-stu-id="8257a-135">As a rule of thumb, the initial measurements count could be approximately $1/\mbox{tolerance}^2$ (see definition of tolerance in the previous paragraph).</span></span> <span data-ttu-id="8257a-136">Gradyanın çok Erratic ve yakınsama çok zor gibi görünüyorsa, birinin ölçüm sayısını yukarı doğru gözden geçirmeniz gerekir.</span><span class="sxs-lookup"><span data-stu-id="8257a-136">One would need to revise the measurement count upward if the gradient descent appears to be too erratic and convergence too hard to achieve.</span></span>

### <a name="training-threads"></a><span data-ttu-id="8257a-137">Eğitim iş parçacıkları</span><span class="sxs-lookup"><span data-stu-id="8257a-137">Training threads</span></span>

<span data-ttu-id="8257a-138">Sınıflandırıcı için eğitim yardımcı programı olan olasılık işlevi, genellikle kalite gereği önemli ölçüde farklı olabilecek parametre alanında çok sayıda yerel optimize eden bir convex.</span><span class="sxs-lookup"><span data-stu-id="8257a-138">The likelihood function which is the training utility for the classifier is very seldom convex, meaning that it usually has a multitude of local optima in the parameter space that may differ significantly by quality.</span></span> <span data-ttu-id="8257a-139">SGD işlemi yalnızca tek bir en uygun değere yakınlaşdığından, birden çok başlangıç parametresi vektörünü incelemek önemlidir.</span><span class="sxs-lookup"><span data-stu-id="8257a-139">Since the SGD process can converge to only one specific optimum, it is important to explore multiple starting parameter vectors.</span></span> <span data-ttu-id="8257a-140">Machine Learning 'de yaygın olarak kullanılan uygulama, başlangıç vektörlerini rastgele başlatmaktır.</span><span class="sxs-lookup"><span data-stu-id="8257a-140">Common practice in machine learning is to initialize such starting vectors randomly.</span></span> <span data-ttu-id="8257a-141">Q#Eğitim API 'si, bu tür başlangıç vektörlerine yönelik rastgele bir dizi kabul eder ancak temeldeki kod bunları sırayla araştırır.</span><span class="sxs-lookup"><span data-stu-id="8257a-141">The Q# training API accepts an arbitrary array of such starting vectors but the underlying code explores them sequentially.</span></span> <span data-ttu-id="8257a-142">Bir çok veya daha fazla paralel bilgi işlem mimarisi üzerinde Q# , çağrılar genelinde farklı parametre başlatmaları ile paralel olarak EĞITIM API 'sine birkaç çağrı gerçekleştirmeniz önerilir.</span><span class="sxs-lookup"><span data-stu-id="8257a-142">On a multicore computer or in fact on any parallel computing architecture it is advisable to perform several calls to Q# training API in parallel with different parameter initializations across the calls.</span></span>

#### <a name="how-to-modify-the-hyperparameters"></a><span data-ttu-id="8257a-143">Hiper parametreleri değiştirme</span><span class="sxs-lookup"><span data-stu-id="8257a-143">How to modify the hyperparameters</span></span>

<span data-ttu-id="8257a-144">QML kitaplığında, hiper parametreleri değiştirmek için en iyi yol UDT 'nin varsayılan değerlerini geçersiz kılmasından oluşur [`TrainingOptions`](xref:microsoft.quantum.machinelearning.trainingoptions) .</span><span class="sxs-lookup"><span data-stu-id="8257a-144">In the QML library, the best way to modify the hyperparameters is by overriding the default values of the UDT [`TrainingOptions`](xref:microsoft.quantum.machinelearning.trainingoptions).</span></span> <span data-ttu-id="8257a-145">Bunu yapmak için, işlevi ile çağırır [`DefaultTrainingOptions`](xref:microsoft.quantum.machinelearning.defaulttrainingoptions) ve `w/` varsayılan değerleri geçersiz kılmak için işlecini uygular.</span><span class="sxs-lookup"><span data-stu-id="8257a-145">To do this we call it with the function [`DefaultTrainingOptions`](xref:microsoft.quantum.machinelearning.defaulttrainingoptions) and apply the operator `w/` to override the default values.</span></span> <span data-ttu-id="8257a-146">Örneğin, 100.000 ölçülerini ve 0,01 öğrenme oranını kullanmak için:</span><span class="sxs-lookup"><span data-stu-id="8257a-146">For example, to use 100,000 measurements and a learning rate of 0.01:</span></span>
 ```qsharp
let options = DefaultTrainingOptions()
w/ LearningRate <- 0.01
w/ NMeasurements <- 100000;
 ```
